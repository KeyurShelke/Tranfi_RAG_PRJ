ğŸ“Œ Project Structure

TransFi/
â”‚
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ ingest_core.py # Reusable ingestion pipeline (async)
â”‚ â”œâ”€â”€ query_core.py # Reusable RAG + retrieval logic
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_html/ # Saved HTML pages
â”‚ â”œâ”€â”€ text/ # Cleaned text from pages
â”‚
â”œâ”€â”€ index/
â”‚ â”œâ”€â”€ embeddings.npy # Vector index
â”‚ â”œâ”€â”€ metadata.json # Chunk + doc metadata
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ api.py # FastAPI service
â”‚ â”œâ”€â”€ webhook_receiver.py # Webhook callback server
â”‚
â”œâ”€â”€ ingest.py # CLI ingestion tool (Part 1)
â”œâ”€â”€ query.py # CLI query tool (Part 1)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

âš¡ Features

âœ… Part 1 â€” CLI RAG Pipeline

Async web scraping using aiohttp

HTML â†’ cleaned text â†’ chunks

Batch async embeddings with Ollama

Vector search using cosine similarity

LLM answer generation with Ollama llama3

Pretty metrics + source citations

-

âœ… Part 2 â€” FastAPI RAG Service

/api/ingest â†’ background ingestion job

/api/query â†’ single-question RAG

/api/query/batch â†’ multi-question async RAG

Webhook-based ingestion completion callback

Fully async architecture using asyncio

---

ğŸ›  Installation

## 1ï¸âƒ£ Clone the repo

git clone <git@github.com:KeyurShelke/Tranfi_Rag.git>
cd TRANSFI_PROJECT

-

## 2ï¸âƒ£ Create virtual environment

python3 -m venv .venv
source .venv/bin/activate # macOS/Linux

-

## 3ï¸âƒ£ Install dependencies

## pip install -r requirements.txt

4ï¸âƒ£ Install and run Ollama
Download Ollama â†’ https://ollama.com/download
Then pull required models:

- ollama pull nomic-embed-text
  ollama pull llama3
- ***

ğŸ§© PART 1 â€” CLI Ingestion & Querying

ğŸ“¥ 1. Run Ingestion
This crawls TransFi pages, cleans text, chunks them, embeds using Ollama, and stores the index.

- python ingest.py --url https://www.transfi.com --concurrency 8
-

Example Output
=== Ingestion Metrics ===
Total Time (s): 82.01
Pages Scraped: 18
Pages Failed: 0
Total Chunks Created: 477
Embedding Generation Time (s): 79.24
Saved embeddings -> index/embeddings.npy
Saved metadata -> index/metadata.json

## ğŸ” 2. Run Query (Single)

## python query.py --question "What is BizPay?"

## ğŸ” 3. Run Query (Batch)

## python query.py --questions questions.txt --concurrent

Output Example
QUESTION: What is BizPay?

Based on the provided context...

--- SOURCES ---
[1] https://www.transfi.com/products/bizpay
Snippet: Unlock the world of borderless payments...

--- METRICS ---
Total Latency (s): 63.15
Embedding Time (s): 0.004
Retrieval Time (s): 0.002
LLM Time (s): 63.10

---

ğŸš€ PART 2 â€” FastAPI Service

The system now exposes ingestion + query endpoints via REST APIs.

## ğŸ–¥ï¸ Run Webhook Receiver (Terminal 1)

## python webhook_receiver.py --port 8001

Expected:

ğŸš€ Webhook Receiver running on http://localhost:8001/webhook
Timestamp: ...
Payload: { "metrics": {...} }

---

## ğŸŒ Run FastAPI Server (Terminal 2)

## uvicorn api:app --port 8000 --reload

Expected:

Uvicorn running on http://127.0.0.1:8000

---

## ğŸ“¡ Trigger Ingestion (Terminal 3)

curl -X POST http://localhost:8000/api/ingest \
 -H "Content-Type: application/json" \
 -d '{"urls": ["https://www.transfi.com"], "callback_url": "http://localhost:8001/webhook"}'

-

Immediate API response:

{"message": "Ingestion started", "will_callback_to": "http://localhost:8001/webhook"}

Later in Terminal 1:

Webhook received! { "metrics": {...} }

---

## Query Endpoint

curl -X POST http://localhost:8000/api/query \
 -H "Content-Type: application/json" \
 -d '{"question": "What is BizPay?"}'

-

Returns:

{
"answer": "...",
"sources": [...],
"metrics": {...}
}

---

ğŸ”¥ Batch Query Endpoint
Sync mode:

- curl -X POST http://localhost:8000/api/query/batch \
   -H "Content-Type: application/json" \
   -d '{"questions": ["Q1","Q2"]}'
-

## Async mode (webhook):

curl -X POST http://localhost:8000/api/query/batch \
 -H "Content-Type: application/json" \
 -d '{"questions": ["Q1","Q2"], "callback_url": "http://localhost:8001/webhook"}'

- ***

Architecture Overview
ğŸ— Layered Design

API Layer (FastAPI)
â†“
Service Layer (Part 2 logic)
â†“
Core Logic (Part 1 ingestion + RAG)
â†“
Ollama (Embeddings + LLM)

Async Everywhere

aiohttp for HTTP fetch + embeddings

asyncio for parallel tasks

FastAPI BackgroundTasks for ingestion

Webhooks for long-running job completion

ğŸ Conclusion

This project implements a complete RAG pipeline with:

robust async ingestion

reproducible vector search

FastAPI microservice architecture

webhook-based decoupled execution
